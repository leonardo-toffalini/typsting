#import "@preview/thmbox:0.3.0": *
#show: thmbox-init()

#set text(font: "Times New Roman")
#set page(numbering: "1")

#let exercise-counter = counter("exercise")
#show: sectioned-counter(exercise-counter, level: 1)
#let exercise = exercise.with(counter: exercise-counter)

#let solution = proof.with(
    title: "Solution", 
)

#align(center)[
  #text(blue, size: 25pt)[*Homework 3*] \

  Toffalini Leonardo
]


#exercise[
  Prove that if $f: RR^n -> RR$ is a convex function which is $G$-Lipschitz
  continuous, then $norm(nabla f(x)) <= G$ for all $x in RR^n$.
]

#solution[
  By the first order characterization of convex functions:
  $
    f(x) + nabla f(x) dot (y - x) <= f(y).
  $
  Since $f$ is $G$-Lipschitz:
  $
    norm(f(y) - f(x)) <= G dot norm(y - x).
  $

  Reordering the FOC we get the following:
  $
    nabla f(x) dot (y - x) &<= f(y) - f(x) \
    norm(nabla f(x) dot (y - x)) &<= norm(f(y) - f(x)) \
    norm(nabla f(x)) dot norm((y - x)) &<= norm(f(y) - f(x)) \
    norm(nabla f(x)) &<= norm(f(y) - f(x))/norm((y - x)) <= G  \
  $
]

#exercise[
  Give an example of a function $f: RR^n -> RR$ such that $norm(nabla f(x))_2
  <= 1$ for all $x in RR^n$ but the Lipschitz constant of its gradient is
  unbounded.
]

#solution[
  Basically we have to give a function $F$ that is lies in the $[-1, 1]$ strip, but
  is not Lipschitz continuous, then find a function $f$ for which $nabla f = F$.

  Let's just restrict ourselves to function of the form $F: RR->RR$. A classic
  example of a bounded function that is not Lipschitz is $F(x) = sin(1\/x)$.
  Clearly $F(x) in [0, 1]$ for all $x in RR$. However near $0$ it is not
  Lipschitz.

  Now we just have to give a function such that $f'(x) = F(x)$, so just solve
  the following differential equation:
  $
    f'(x) &= sin (1/x) \
    f(x) &= integral_0^x sin(1/t) dif t
  $

  By the Newton--Leibniz formula $f'(x) = sin (1\/x)$ if we define $sin (1\/t)
  = 0$ for $t = 0$.
]

#pagebreak()

#exercise[
  Let $f(x) = 1/2 x^2$. Run a gradient descent with step size $alpha in (0,
  2)$, that is $x_(t+1) = x_t - alpha nabla f(x_t)$.
  + Write the iteration explicitly.
  + For which values of $alpha$ does the sequence converge to $0$?
]

#solution[
  Let $x_0 = 2$ and $alpha = 0.5$.
  $
    x_1 &= x_0 - alpha nabla f(x_0) \
    x_1 &= 2 - 0.5 dot nabla (1/2 x_0^2) \
    x_1 &= 2 - 0.5 dot 2 \
    x_1 &= 1
  $

  For all values of $alpha in (0, 2)$ the method converges to $x^* = 0$. Since
  our only problem would arise if we have such a large step size that we end up
  higher on the other side of the parabola than where we started. Because, of
  course, we can only end up farther from the optimum on the otherside, since
  we always step towards the optimum.

  This situation could only arise if $abs(x_k - alpha x_k) > abs(x_k)$, which
  is only true when $alpha > 2$ since $alpha$ is positive.
]

#pagebreak()

#exercise[
  Suppose $f$ is $mu$-strongly convex with $L$-Lipschitz continuous gradient. Prove that
  gradient descent with step size $alpha in (0, 2\/L)$ satisfies the linear
  convergence bound
  $
    norm(x_(k+1) - x^*)^2 <= (1 - alpha mu) norm(x_k - x^*)^2.
  $
  What is the optimal choice of $alpha$?
]

#solution[
  Let us denote $x_k - x^* = A$ and $nabla f(x_k) = G$ for easier notation.

  We can rewrite the left hand side of the desired inequality by substituing
  for the definition of how we got $x_(k+1)$ with a single gradient step:
  $
    norm(x_(k+1) - x^*)^2 = norm(x_k - alpha nabla f(x_k) - x^*)^2 = norm(A -
    alpha G)^2.
  $

  We can expand the norm square with a dot product as follows:
  $
    norm(A - alpha G)^2 = norm(A)^2 - 2 alpha (A dot G) + alpha^2 norm(G)^2.
  $

  #set math.equation(numbering: "(1)")
  Now we just need to bound $A dot G$ by some values, for which we state two
  results:
  $
    G dot A >= mu norm(A)^2,
  $<first-stmt>

  $
    G dot A >= 1/L norm(G)^2.
  $<second-stmt>

  #set math.equation(numbering: none)

  With the above two results we can bound the above equation as follows:
  $
    norm(A - alpha G)^2 &= norm(A)^2 - alpha (A dot G) - alpha (A dot G) +
  alpha^2 norm(G)^2 \
    &<= norm(A)^2 - alpha mu norm(A)^2 - alpha/L norm(G)^2 + alpha^2 norm(G)^2 \
    &= (1 - alpha mu) norm(A)^2 + alpha (alpha - 1/L) norm(G)^2.
  $

  For $alpha in (0, 1/L)$
  $
    alpha (alpha - 1/L) norm(G)^2 < 0,
  $
  so if we subtract it from the right hand side we get something that is
  greater or equal:
  $
    norm(A - alpha G)^2 <= (1 - alpha mu) norm(A)^2,
  $
  which is exactly what we were hoping to prove.
]

#pagebreak()

#exercise[
  Let $f: RR^n -> RR$ be (possibly nonconvex) differentiable with $L$-Lipschitz
  continuous gradient, and let ${x_k}$ be generated by gradient descent with
  constant step size $alpha = 1\/L$. Show that after $K$ iterations
  $
    min_(0 <= k <= K - 1) norm(nabla f(x_k))^2 <= (2L(f(x_0) - inf f))/K.
  $
]

#solution[
  Lemma 6.3 from the Vishnoi Convex Optimization book states the following:
  $
    f(y) - f(x) - nabla f(x) dot (y - x) <= L/2 norm(x - y)^2,
  $
  for a function with an $L$-Lipschitz gradient. Notice, that the lemma does not
  rely on $f$ being convex.

  We can rewrite it as follows:
  $
    f(y) <= f(x) + nabla f(x) dot (y - x) + L/2 norm(y - x)^2.
  $
  Let $y$ be the result of a single gradient step from $x$, meaning $y = x -
  alpha nabla f(x)$, and plug this into the above inequality:
  $
    f(x - alpha nabla f(x)) &<= f(x) + nabla f(x) dot (- 1/L nabla f(x)) + L/2
  norm(-1/L nabla f(x))^2 \
    &= f(x) - 1/L norm(nabla f(x))^2 + 1/(2L) norm(nabla f(x))^2 \
    &= f(x) - 1/(2L) norm(nabla f(x))^2.
  $

  Now, writing $x = x_k$ and thus $y = x_(k+1)$ we arrive at the following:
  $
    norm(nabla f(x_k))^2 <= 2 L (f(x_k) - f(x_(k+1))).
  $

  With the gradient at a single point bounded by above, we are almost at the
  finish line, we just need to bound the minimum by above:
  $
    min_(0 <= k <= K - 1) norm(nabla f(x_k))^2 <= 1/K sum_(k=0)^(K-1)
    <= (2 L)/K sum_(k=0)^(K-1)(f(x_k) - f(x_(k+1))).
  $
  Notice, that the rightmost sum is telescoping and all that remains is the
  first and last elements:
  $
    = (2L)/K (f(x_0) - f(x_K)).
  $
  For the last step we just realize that $inf f < f(x_K)$, so
  $
    min_(0 <= k <= K - 1) norm(nabla f(x_k))^2 <= (2L)/K (f(x_0) - inf f).
  $
]

#pagebreak()

#exercise[
  Consider the quadratic function
  $
    f(x, y) = 1/2 (3x^2 + 2 x y + 4 y^2).
  $
  + Write down its gradient and compute the Lipschitz constant $L$ of the gradient.
  + Perform one step of gradient descent with step size $alpha = 1 \/ L$,
    starting from $(x_0, y_0) = (1, 1)$.
]

#solution[
  $
    nabla f(x, y) = mat(
      3x + 1y;
      1x + 4y;
      delim: "["
    )
  $

  We need to find an $L > 0$ such that
  $
    norm(nabla f(x_1, y_1) - nabla f(x_2, y_2)) <= L norm((x_1, y_1) - (x_2,
    y_2)).
  $
  Expanding the gradient we arrive at the following inequality that gives us $L$:
  $

    norm(
    mat(
      3x_1 + 1y_1;
      1x_1 + 4y_1;
      delim: "["
    ) - 
    mat(
      3x_2 + 1y_2;
      1x_2 + 4y_2;
      delim: "["
    )
  ) &=
  norm(
    mat(
      3(x_1 - x_2) + 1(y_1 - y_2);
      1(x_1 - x_2) + 4(y_1 - y_2);
      delim: "["
    )
    ) \
  &= norm(
    mat(
      3, 1;
      1, 4;
      delim: "["
    ) dot mat(
      x_1 - x_2;
      y_1 - y_2;
      delim: "["
    )
    )\ &<= 
    norm(mat(
      3, 1;
      1, 4;
      delim: "["
    ))
    dot norm(
    mat(
      x_1 - x_2;
      y_1 - y_2;
      delim: "["
    )
  ).
  $

  Thus $L = norm(mat(
      3, 1;
      1, 4;
      delim: "["
    ))$. For simplicity lets denote this matrix by $A$. We know that the
  $2$-norm of a matrix is calculated as follows:
  $
    norm(A)_2 = sqrt(lambda_("max") (A A^T)).
  $
  This calculation is quite tiresome. However, if we are free to choose the
  norm, we can just choose the infinity norm, for which $L = norm(A)_infinity =
  5$.
]

#solution[
  Let $z = (x, y)$, thus $z_0 = (x_0, y_0) = (1, 1)$.
  $
    z_1 &= z_0 - nabla f(z_0) \
    &= mat(
    1;
    1;
    delim: "["
  ) - 1/5 dot mat(
    3 + 1;
    1 + 4;
    delim: "["
  )
  = mat(
    1 - 0.8;
    1 - 1;
    delim: "["
  )
  = mat(
    -0.2;
    0;
    delim: "["
  )
  $
]

#pagebreak()

#align(center)[
  #text(size: 18pt)[*Supplementary proofs*] \
]
#proposition[
  If $f$ is a $mu$-strongly convex function, then
  $
    (nabla f(x) - nabla f(y)) dot (x - y) >= mu norm(x - y)^2.
  $
]

#proof[
  Write out the definition of $mu$-strong convexity in both directions:
  $
    f(y) &>= f(x) + nabla f(x) dot (y - x) + mu/2 norm(y - x)^2 \
    f(x) &>= f(y) + nabla f(y) dot (x - y) + mu/2 norm(x - y)^2.
  $

  Summing these two inequalities we get the following:
  $
    f(x) + f(y) &>= f(y) + f(x) + nabla f(x) dot (y - x) + nabla f(y) dot (x -
  y) + mu norm(x - y)^2 \
    0 &>= (nabla f(x) - nabla f(y)) dot (y - x) + mu norm(x - y)^2 \
    (nabla f(x) - nabla f(y)) dot (x - y) &>= mu norm(x - y)^2.
  $

  With this we proved the proposition, and we can see that @first-stmt
  used in our solution can be achieved by the following variable assignment $x
  = x_k$ and $y = x^*$.
]

#proposition[Baillon--Haddad][
  If $f$ has an $L$-Lipschitz continuous gradient and is defined on a convex
  domain, then
  $
    (nabla f(x) - nabla f(y)) dot (x - y) >= 1/L norm(nabla f(x) - nabla f(y))^2.
  $
]

#proof[
  The proof is quite involved and this is a somewhat well-known result so we
  just present the outline of the proof and how @second-stmt follows from this
  proposition. The following blog post has a great explanation of this result:
  #link("https://samuelvaiter.com/a-first-look-at-convex-analysis/")

  1.
  $
    (nabla f(x) - nabla f(y)) dot (x - y) <=^"C.S." norm(nabla f(x) - nabla
    f(y)) dot norm(x - y) <= L norm(x - y)^2
  $

  2. From the previous we can prove the following:
  $
    f(y) <= f(x) + nabla f(x) dot (y - x) + L/2 norm(x - y)^2
  $

  3. By the above result we can bound the difference of any function value to
     the optimum value:
  $
    1/(2L) norm(nabla f(z))^2 <= f(z) - f(x^*) <= L/2 norm(z - x^*)^2,
  $
  where $x^*$ is the minimizer of $f$.

  4.
  $
    f(y) - f(x) - nabla f(x) dot (x - y) >= 1/(2L) norm(nabla f(y) - nabla f(y))^2
  $

  5. Writing the previous inequality once for $x$ and once for $y$ and summing
     them up we arrive at our destination:
  $
    (nabla f(x) - nabla f(y)) dot (x - y) >= 1/L norm(nabla f(x) - nabla f(y))^2.
  $

  We can see that @second-stmt which we relied on in our solution is
  achieved by the variable substitution $x = x_k$ and $y = x^*$, since $nabla
  f(x^*) = 0$ because it is the minimizer.
]
